= AI Assistant plugin
:navtitle: AI Assistant
:description: Sends queries to registered AI APIs and sets the results in the document or temporarily stores the results.
:description_short: Query AI APIs and accept results.
:keywords: plugin, ai, artificial intelligence, query, search, llm, gpt, assistant
:pluginname: AI Assistant
:plugincode: ai
:plugincategory: premium

include::partial$misc/admon-ai-pricing.adoc[]

include::partial$misc/admon-requires-6.6v.adoc[]

The {pluginname} plugin allows a user to interact with registered AI APIs by sending queries and viewing responses within a {productname} editor dialog.

Once a response is generated and displayed within the dialog, the user can choose to either:

. insert it into the editor at the current selection;
. create another query to further refine the response generated by the AI; or
. close the dialog and discard the returned response.

Users can retrieve a history of their conversations with the AI using the xref:#getThreadLog[`getThreadLog` API], including any discarded responses.

== Interactive example

NOTE: This example uses a proxy endpoint to communicate with the OpenAI API. This is done to avoid exposing the API key in the client-side code. For more information on using a proxy server with the {pluginname} plugin, see the xref:ai-proxy.adoc[AI Proxy Server reference guide].

liveDemo::ai[]

== Basic setup

To add the {pluginname} plugin to the editor, follow these steps:

- Add `{plugincode}` to the `plugins` option in the editor configuration.
- Add the `ai_request` function to the editor configuration.

For example:

[source,js]
----
tinymce.init({
  selector: 'textarea',  // change this value according to your HTML
  plugins: 'ai',
  toolbar: 'aidialog aishortcuts',
  ai_request: <AI_REQUEST_FUNCTION>,
});
----

== Using a proxy server with {pluginname}

NOTE: As per OpenAIâ€™s https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety[best practices for API key safety], deployment of an API key in a client-side environment is specifically not recommended.

Using a proxy server obviates this, reducing financial and service uptime risks.

A proxy server can also provide flexibility by allowing extra processing before the request is sent to an LLM AI endpoint and before returning the response to the user.

See the xref:ai-proxy.adoc[AI Proxy Server reference guide] for information on how to setup a proxy server for use with the {pluginname}.

NOTE: The xref:ai-proxy.adoc[AI Proxy Server reference guide] is, as its name notes, a reference. There is no single proxy server setup that is right or correct for all circumstances and other setups may be better for your use-case.


== Options

The following configuration options affect the behavior of the {pluginname} plugin.

include::partial$configuration/ai_request.adoc[leveloffset=+1]

include::partial$configuration/ai_shortcuts.adoc[leveloffset=+1]

include::partial$misc/plugin-toolbar-button-id-boilerplate.adoc[]

include::partial$misc/plugin-menu-item-id-boilerplate.adoc[]

== Commands

The {pluginname} plugin provides the following {productname} commands.

include::partial$commands/{plugincode}-cmds.adoc[]

== Events

The {pluginname} plugin provides the following events.

include::partial$events/{plugincode}-events.adoc[]

== APIs

The {pluginname} plugin provides the following APIs.

include::partial$plugin-apis/{plugincode}-apis.adoc[]
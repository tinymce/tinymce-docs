= AI Assistant plugin
:navtitle: AI Assistant
:description: Sends queries to registered AI APIs and sets the results in the document or temporarily stores the results for later use.
:description_short: Query AI APIs and accept results.
:keywords: plugin, ai, artificial intelligence, query, search.
:pluginname: AI Assistant
:plugincode: ai
:plugincategory: premium

include::partial$misc/admon-ai-pricing.adoc[]

include::partial$misc/admon-requires-6.6v.adoc[]

The {pluginname} allows a {productname} document to send queries to registered AI APIs and either:

. add the returned results to a document; or
. temporarily store the returned results for later use.

[IMPORTANT]
.On the absence of an AI Assistant demo
====
This initial release of the AI Assistant developer documentation does not include an in-page working demo.

The logistics of setting up such a demo are still being investigated.

When a solution that is both useful and workable is found, it will replace this notice.
====

== Basic setup

To add the {pluginname} plugin to the editor, add both `{plugincode}` to the `plugins` option in the editor configuration and the `ai_request` function to the editor configuration.

For example:

[source,js]
----
tinymce.init({
  selector: 'textarea',  // change this value according to your HTML
  plugins: 'ai',
  toolbar: 'aidialog aishortcuts',
  ai_request: (request, respondWith) => { }
});
----

[IMPORTANT]
.Default strings added to all prompts
====
The {pluginname} automatically appends default strings to all prompts.

Depending on the prompt, the added string will be one of the following:

[source,text]
----
Answer the question based on the context below.
The response should be in HTML format.
The response should preserve any HTML formatting, links, and styles in the context.
----

These added strings both improve the UX and increase the response accuracy.

However, these added strings do increase token use.
====

== Using a proxy server with {pluginname}

A proxy server can provide flexibility by allowing extra processing before the request is sent to an LLM AI endpoint and before returning the response to the user.

See the xref:ai-proxy.adoc[AI Proxy Server reference guide] for information on how to setup a proxy server for use with the {pluginname}.

NOTE: The xref:ai-proxy.adoc[AI Proxy Server reference guide] is, as its name notes, a reference. There is no single proxy server setup that is right or correct for all circumstances and other setups may be better for your use-case.


== Options

The following configuration options affect the behavior of the {pluginname} plugin.

include::partial$configuration/ai_request.adoc[][leveloffset=+1]

[ai_shortcuts]
include::partial$configuration/ai_shortcuts.adoc[][leveloffset=+1]

include::partial$misc/plugin-toolbar-button-id-boilerplate.adoc[]

include::partial$misc/plugin-menu-item-id-boilerplate.adoc[]

== Commands

The {pluginname} plugin provides the following {productname} commands.

include::partial$commands/{plugincode}-cmds.adoc[]

== Events

The {{pluginname}} plugin provides the following events.

include::partial$events/{plugincode}-events.adoc[]

== APIs

The {{pluginname}} plugin provides the following APIs.

include::partial$plugin-apis/{plugincode}-apis.adoc[]

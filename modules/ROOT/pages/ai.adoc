= AI Assistant plugin
:navtitle: AI Assistant
:description: Sends queries to registered AI APIs and sets the results in the document or temporarily stores the results.
:description_short: Query AI APIs and accept results.
:keywords: plugin, ai, artificial intelligence, query, search.
:pluginname: AI Assistant
:plugincode: ai
:plugincategory: premium

include::partial$misc/admon-ai-pricing.adoc[]

include::partial$misc/admon-requires-6.6v.adoc[]

The {pluginname} plugin allows a user to interact with registered AI APIs by sending queries and viewing responses within a {productname} editor dialog.

Once a response is generated and displayed within the dialog, the user can choose to either:

. insert it into the editor content, at the current selection;
. type another query to further refine the response generated by the AI; or
. discard or close the dialog.

Users can retrieve a history of their conversations with the AI using the xref:#getThreadLog[`getThreadLog` API].

[IMPORTANT]
.On the absence of an {pluginname} demo
====
This initial release of the {pluginname} developer documentation does not include an in-page working demo.
====

== Basic setup

To add the {pluginname} plugin to the editor, add both `{plugincode}` to the `plugins` option in the editor configuration and the `ai_request` function to the editor configuration.

For example, interfacing with the OpenAI Completions API:

[source,js]
----
const api_key = '<INSERT_API_KEY_HERE>';

tinymce.init({
  selector: 'textarea',  // change this value according to your HTML
  plugins: 'ai',
  toolbar: 'aidialog aishortcuts',
  ai_request: (request, respondWith) => {
    const openAiOptions = {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${api_key}`
      },
      body: JSON.stringify({
        model: 'gpt-3.5-turbo',
        temperature: 0.7,
        max_tokens: 800,
        messages: [{ role: 'user', content: request.prompt }],
      })
    };
    respondWith.string((signal) => window.fetch('https://api.openai.com/v1/chat/completions', { signal, ...openAiOptions })
      .then(async (response) => {
        if (response.ok) {
          return response.json();
        } else {
          return Promise.reject(`Failed to communicate with the OpenAI API. ${await response.text()}`);
        }
      })
      .then((data) =>
        data.error
        ? Promise.reject(`Failed to communicate with the ChatGPT API because of ${data.error.type} error: ${data.error.message}`)
        : data)
      .then((data) =>
        // Extract the response content from the data returned by the API
        data?.choices[0]?.message?.content?.trim()
      )
    );
  }
});
----

== Using a proxy server with {pluginname}

A proxy server can provide flexibility by allowing extra processing before the request is sent to an LLM AI endpoint and before returning the response to the user.

See the xref:ai-proxy.adoc[AI Proxy Server reference guide] for information on how to setup a proxy server for use with the {pluginname}.

NOTE: The xref:ai-proxy.adoc[AI Proxy Server reference guide] is, as its name notes, a reference. There is no single proxy server setup that is right or correct for all circumstances and other setups may be better for your use-case.


== Options

The following configuration options affect the behavior of the {pluginname} plugin.

include::partial$configuration/ai_request.adoc[][leveloffset=+1]

[ai_shortcuts]
include::partial$configuration/ai_shortcuts.adoc[][leveloffset=+1]

include::partial$misc/plugin-toolbar-button-id-boilerplate.adoc[]

include::partial$misc/plugin-menu-item-id-boilerplate.adoc[]

== Commands

The {pluginname} plugin provides the following {productname} commands.

include::partial$commands/{plugincode}-cmds.adoc[]

== Events

The {{pluginname}} plugin provides the following events.

include::partial$events/{plugincode}-events.adoc[]

== APIs

The {{pluginname}} plugin provides the following APIs.

include::partial$plugin-apis/{plugincode}-apis.adoc[]

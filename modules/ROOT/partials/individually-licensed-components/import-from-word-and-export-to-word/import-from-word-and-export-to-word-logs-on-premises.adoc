[[logs]]
== Logs

The logs from {pluginname} On-Premises are written to `stdout` and `stderr`. Most of them are formatted in JSON. They can be used for monitoring or debugging purposes. In production environments, It is recommend storing the logs to files or using a distributed logging system (like ELK or CloudWatch).

=== Monitoring {pluginname} with logs

To get more insight into how the {pluginname} On-Premises is performing, logs can be used for monitoring. To enable these, add the `ENABLE_METRIC_LOGS=true` environment variable.

=== Log structure

The log structure contains the following information:

* `handler`: A unified identifier of action. Use this field to identify calls.
* `traceId`: A unique RPC call ID.
* `tags`: A semicolon-separated list of tags. Use this field to filter metrics logs.
* `data`: An object containing additional information. It might vary between different transports.
* `data.duration`: The request duration in milliseconds.
* `data.transport`: The type of the request transport. It could be http or ws (websocket).
* `data.status`: The request status. It can be equal to success, fail, warning.
* `data.statusCode`: The response status in HTTP status code standard.

Additionally, for the HTTP transport, the following information is included:

* `data.url`: The URL path.
* `data.method`: The request method.

In case of an error, `data.status` will be equal to `failed` and `data.message` will contain the error message.

An example log for HTTP transport:

[source]
----
{
  "level": 30,
  "time": "2021-03-09T11:15:09.154Z",
  "msg": "Request summary",
  "handler": "postConvert",
  "traceId": "85f13d92-57df-4b3b-98bb-0ca41a5ae601",
  "data": {
    "duration": 752,
    "transport": "http",
    "statusCode": 200,
    "status": "success",
    "url": "/v2/convert/html-docx",
    "method": "POST"
  },
  "tags": "metrics"
}
----

=== Docker

The docker has built-in logging mechanisms that capture logs from the output of the containers. The default logging driver writes the logs to files.

When using this driver, use the `docker logs` command to show logs from a container. Use the `-f` flag to view logs in real time. Refer to the link:https://docs.docker.com/engine/reference/commandline/logs/[official Docker documentation] for more information about the logs command.

[NOTE]
When a container is running for a long period of time, the logs can take up a lot of space. To avoid this problem, you should make sure that the log rotation is enabled. This can be set with the `max-size` option.

=== Distributed logging

If running more than one instance of {pluginname} On-Premises, It is recommend using a distributed logging system. It allows for viewing and analyzing logs from all instances in one place.

=== AWS CloudWatch and other cloud solutions

If running {pluginname} On-Premises in the cloud, the simplest and recommended way is to use a service that is available at the selected provider.

Here are some of the available services:

* AWS: link:https://aws.amazon.com/CloudWatch[CloudWatch^]
* Google Cloud: link:https://cloud.google.com/logging[Cloud Logging^]
* Azure: link:https://azure.microsoft.com/en-us/services/monitor/[Azure Monitor^]

To use CloudWatch with AWS ECS, a log group must be created before, and the log driver must be changed to `awslogs`. When the log driver is configured properly, logs will be streamed directly to CloudWatch.

The `logConfiguration` may look similar to this:

[source, json]
----
"logConfiguration": {
  "logDriver": "awslogs",
  "options": {
    "awslogs-region": "us-west-2",
    "awslogs-group": "tinysource",
    "awslogs-stream-prefix": "tiny-docx-converter-logs"
  }
}
----

Refer to the link:https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_awslogs.html[Using the awslogs Log Driver] article for more information.

=== On-Premises solutions

If using a specific infrastructure such as your own or for some reason cannot use the service offered by a provider, some on-premises distributed logging system can be used.

There are a lot of solutions available, including:

* link:https://www.elastic.co/what-is/elk-stack[ELK^] + link:https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-getting-started.html[Filebeat^]

This is a stack built on top of Elasticsearch, Logstash and Kibana. In this configuration, Elasticsearch stores logs, Filebeat reads logs from Docker and sends them to Elasticsearch, and Kibana is used to view them. Logstash is not necessary because logs are already structured.

* link:https://www.fluentd.org/[Fluentd^]

It uses a dedicated link:https://docs.docker.com/config/containers/logging/fluentd[Docker log driver^] to send the logs. It has a built-in frontend, but can also be integrated with Elasticsearch and Kibana for better filtering.

* link:https://www.graylog.org/[Graylog^]

It uses a dedicated link:https://docs.docker.com/config/containers/logging/gelf[Docker^] log driver to send the logs. It has a built-in frontend and needs Elasticsearch to store the logs as well as a MongoDB database to store the configuration.

==== Example configuration

The example configuration uses Fluentd, Elasticsearch and Kibana to capture logs from Docker.

Before running {pluginname} On-Premises, prepare the logging services. For the purposes of this example, Docker Compose is used. Create the `fluentd`, `elasticsearch` and `kibana` services inside the `docker-compose.yml` file:

[source, yaml]
----
version: '3.7'
services:
  fluentd:
    build: ./fluentd
    volumes:
      - ./fluentd/fluent.conf:/fluentd/etc/fluent.conf
    ports:
      - "24224:24224"
      - "24224:24224/udp"

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:6.8.5
    expose:
      - 9200
    ports:
      - "9200:9200"

  kibana:
    image: docker.elastic.co/kibana/kibana:6.8.5
    environment:
      ELASTICSEARCH_HOSTS: "http://elasticsearch:9200"
    ports:
      - "5601:5601"
----

To integrate Fluentd with Elasticsearch, you first need to install `fluent-plugin-elasticsearch` in the Fluentd image. To do this, create a `fluentd/Dockerfile` with the following content:

[source, dockerfile]
----
FROM fluent/fluentd:v1.10-1

USER root

RUN apk add --no-cache --update build-base ruby-dev \
  && gem install fluent-plugin-elasticsearch \
  && gem sources --clear-all
----

Next, configure the input server and connection to Elasticsearch in the `fluentd/fluent.conf` file:

[source, xml]
----
<source>
  @type forward
  port 24224
  bind 0.0.0.0
</source>
<match *.**>
  @type copy
  <store>
    @type elasticsearch
    host elasticsearch
    port 9200
    logstash_format true
    logstash_prefix fluentd
    logstash_dateformat %Y%m%d
    include_tag_key true
    type_name access_log
    tag_key @log_name
    flush_interval 1s
  </store>
  <store>
    @type stdout
  </store>
</match>
----

The services are now ready to run:

[source, bash]
----
docker-compose up --build
----

When the services are ready, start the {pluginname} On-Premises.

[source, bash, subs="attributes+"]
----
docker run --init -p 8080:8080 \
--log-driver=fluentd \
--log-opt fluentd-address=[Fluentd address]:24224 \
[Your config here] \
{dockerimageimportfromwordexporttoword}:[version]
----

* Open Kibana in your browser.
** It is available at link:http://localhost:5601/[http://localhost:5601/].
* During the first run, you may be asked about creating an index.
* Use the `fluentd-*` pattern and press the “Create” button.
* After this step, the logs should appear in the “Discover” tab.